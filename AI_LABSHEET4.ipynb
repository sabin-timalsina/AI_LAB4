{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMo/B1qsLSM5NWkwarAvme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabin-timalsina/AI_LAB4/blob/main/AI_LABSHEET4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI LABSHEET 4: Artificial Neural Networks â€“ Perceptron\n",
        "\n",
        "**Name:** Sabin Timalsina  \n",
        "**CRN:** 021-369  \n",
        "**Date:** 04/07/2025\n"
      ],
      "metadata": {
        "id": "ppc1cSgRFv8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron for 2-Input Basic Gates (AND/OR)"
      ],
      "metadata": {
        "id": "Nny2PR6THG6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train_perceptron(X, Y, learning_rate=0.1, max_iterations=100):\n",
        "    weights = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(max_iterations):  # max_iterations to avoid infinite loop\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            x = X[i]\n",
        "            y = Y[i]\n",
        "            y_pred = predict(x, weights, bias)\n",
        "            error = y - y_pred\n",
        "\n",
        "            if error != 0:\n",
        "                # Update weights and bias\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += learning_rate * error * x[j]\n",
        "                bias += learning_rate * error\n",
        "                error_found = True\n",
        "\n",
        "        if not error_found:\n",
        "            break  # Stop if no errors (converged)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Input truth table\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "# AND Gate\n",
        "print(\"---- AND Gate ----\")\n",
        "Y_and = [0, 0, 0, 1]\n",
        "w_and, b_and = train_perceptron(X, Y_and)\n",
        "print(f\"Weights: {w_and}, Bias: {b_and}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "# OR Gate\n",
        "print(\"\\n---- OR Gate ----\")\n",
        "Y_or = [0, 1, 1, 1]\n",
        "w_or, b_or = train_perceptron(X, Y_or)\n",
        "print(f\"Weights: {w_or}, Bias: {b_or}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_or, w_or, b_or)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd2TvsgDG4Tu",
        "outputId": "69b6993f-1c67-47df-d245-fc6718bb76d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- AND Gate ----\n",
            "Weights: [0.2, 0.1], Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "---- OR Gate ----\n",
            "Weights: [0.1, 0.1], Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron for n-Input Basic Gates (AND/OR)"
      ],
      "metadata": {
        "id": "pAiCTaqbHoZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train(X, Y, lr=0.1, max_loops=100):\n",
        "    weights = [0.0] * len(X[0])\n",
        "    bias = 0.0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            y_pred = predict(X[i], weights, bias)\n",
        "            error = Y[i] - y_pred\n",
        "            if error != 0:\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += lr * error * X[i][j]\n",
        "                bias += lr * error\n",
        "                error_found = True\n",
        "        if not error_found:\n",
        "            break\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Run for both 3-input and 4-input gates\n",
        "for n in [3, 4]:\n",
        "    print(f\"\\n==== {n}-INPUT GATES ====\")\n",
        "    X = list(product([0, 1], repeat=n))\n",
        "\n",
        "    # AND Gate\n",
        "    Y_and = [int(all(x)) for x in X]\n",
        "    w_and, b_and = train(X, Y_and)\n",
        "    print(f\"\\nAND Gate:\")\n",
        "    print(f\"Weights: {w_and}\")\n",
        "    print(f\"Bias: {b_and}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "    # OR Gate\n",
        "    Y_or = [int(any(x)) for x in X]\n",
        "    w_or, b_or = train(X, Y_or)\n",
        "    print(f\"\\nOR Gate:\")\n",
        "    print(f\"Weights: {w_or}\")\n",
        "    print(f\"Bias: {b_or}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_or, w_or, b_or)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e__lotvaHsdW",
        "outputId": "aa15d9f6-a303-4be3-a7fc-661c68b494e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 3-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n",
            "\n",
            "==== 4-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.4, 0.20000000000000004, 0.1, 0.1]\n",
            "Bias: -0.7999999999999999\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron for Linear Function with 3 Features"
      ],
      "metadata": {
        "id": "NZZh8nMrHz9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset: y = 2x1 + 3x2 - x3 + 5\n",
        "X = []\n",
        "Y = []\n",
        "for _ in range(10):\n",
        "    x1 = random.uniform(0, 1)\n",
        "    x2 = random.uniform(0, 1)\n",
        "    x3 = random.uniform(0, 1)\n",
        "    y = 2 * x1 + 3 * x2 - 1 * x3 + 5\n",
        "    X.append([x1, x2, x3])\n",
        "    Y.append(y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = [0.0, 0.0, 0.0]\n",
        "bias = 0.0\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_true = Y[i]\n",
        "\n",
        "        # Linear output (no activation)\n",
        "        y_pred = sum([x[j] * weights[j] for j in range(3)]) + bias\n",
        "\n",
        "        # Error\n",
        "        error = y_true - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        for j in range(3):\n",
        "            weights[j] += lr * error * x[j]\n",
        "        bias += lr * error\n",
        "\n",
        "        total_error += error ** 2  # squared error\n",
        "\n",
        "    mse = total_error / len(X)\n",
        "    print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Final result\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhG9dzFtH219",
        "outputId": "5bcfecb3-c5ec-4f35-8e53-d8f972fd7c8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 47.8625\n",
            "Epoch 2: MSE = 33.4025\n",
            "Epoch 3: MSE = 23.3947\n",
            "Epoch 4: MSE = 16.4671\n",
            "Epoch 5: MSE = 11.6705\n",
            "Epoch 6: MSE = 8.3480\n",
            "Epoch 7: MSE = 6.0452\n",
            "Epoch 8: MSE = 4.4475\n",
            "Epoch 9: MSE = 3.3374\n",
            "Epoch 10: MSE = 2.5647\n",
            "Epoch 11: MSE = 2.0250\n",
            "Epoch 12: MSE = 1.6466\n",
            "Epoch 13: MSE = 1.3797\n",
            "Epoch 14: MSE = 1.1899\n",
            "Epoch 15: MSE = 1.0533\n",
            "Epoch 16: MSE = 0.9537\n",
            "Epoch 17: MSE = 0.8796\n",
            "Epoch 18: MSE = 0.8233\n",
            "Epoch 19: MSE = 0.7793\n",
            "Epoch 20: MSE = 0.7438\n",
            "Epoch 21: MSE = 0.7144\n",
            "Epoch 22: MSE = 0.6893\n",
            "Epoch 23: MSE = 0.6672\n",
            "Epoch 24: MSE = 0.6472\n",
            "Epoch 25: MSE = 0.6289\n",
            "Epoch 26: MSE = 0.6118\n",
            "Epoch 27: MSE = 0.5956\n",
            "Epoch 28: MSE = 0.5802\n",
            "Epoch 29: MSE = 0.5653\n",
            "Epoch 30: MSE = 0.5510\n",
            "Epoch 31: MSE = 0.5372\n",
            "Epoch 32: MSE = 0.5237\n",
            "Epoch 33: MSE = 0.5106\n",
            "Epoch 34: MSE = 0.4979\n",
            "Epoch 35: MSE = 0.4855\n",
            "Epoch 36: MSE = 0.4734\n",
            "Epoch 37: MSE = 0.4616\n",
            "Epoch 38: MSE = 0.4501\n",
            "Epoch 39: MSE = 0.4389\n",
            "Epoch 40: MSE = 0.4280\n",
            "Epoch 41: MSE = 0.4174\n",
            "Epoch 42: MSE = 0.4070\n",
            "Epoch 43: MSE = 0.3969\n",
            "Epoch 44: MSE = 0.3870\n",
            "Epoch 45: MSE = 0.3774\n",
            "Epoch 46: MSE = 0.3680\n",
            "Epoch 47: MSE = 0.3588\n",
            "Epoch 48: MSE = 0.3499\n",
            "Epoch 49: MSE = 0.3412\n",
            "Epoch 50: MSE = 0.3327\n",
            "Epoch 51: MSE = 0.3245\n",
            "Epoch 52: MSE = 0.3164\n",
            "Epoch 53: MSE = 0.3085\n",
            "Epoch 54: MSE = 0.3009\n",
            "Epoch 55: MSE = 0.2934\n",
            "Epoch 56: MSE = 0.2861\n",
            "Epoch 57: MSE = 0.2790\n",
            "Epoch 58: MSE = 0.2721\n",
            "Epoch 59: MSE = 0.2653\n",
            "Epoch 60: MSE = 0.2587\n",
            "Epoch 61: MSE = 0.2523\n",
            "Epoch 62: MSE = 0.2461\n",
            "Epoch 63: MSE = 0.2400\n",
            "Epoch 64: MSE = 0.2340\n",
            "Epoch 65: MSE = 0.2282\n",
            "Epoch 66: MSE = 0.2226\n",
            "Epoch 67: MSE = 0.2171\n",
            "Epoch 68: MSE = 0.2117\n",
            "Epoch 69: MSE = 0.2064\n",
            "Epoch 70: MSE = 0.2013\n",
            "Epoch 71: MSE = 0.1963\n",
            "Epoch 72: MSE = 0.1915\n",
            "Epoch 73: MSE = 0.1868\n",
            "Epoch 74: MSE = 0.1821\n",
            "Epoch 75: MSE = 0.1776\n",
            "Epoch 76: MSE = 0.1732\n",
            "Epoch 77: MSE = 0.1690\n",
            "Epoch 78: MSE = 0.1648\n",
            "Epoch 79: MSE = 0.1607\n",
            "Epoch 80: MSE = 0.1568\n",
            "Epoch 81: MSE = 0.1529\n",
            "Epoch 82: MSE = 0.1491\n",
            "Epoch 83: MSE = 0.1454\n",
            "Epoch 84: MSE = 0.1419\n",
            "Epoch 85: MSE = 0.1384\n",
            "Epoch 86: MSE = 0.1349\n",
            "Epoch 87: MSE = 0.1316\n",
            "Epoch 88: MSE = 0.1284\n",
            "Epoch 89: MSE = 0.1252\n",
            "Epoch 90: MSE = 0.1221\n",
            "Epoch 91: MSE = 0.1191\n",
            "Epoch 92: MSE = 0.1162\n",
            "Epoch 93: MSE = 0.1133\n",
            "Epoch 94: MSE = 0.1106\n",
            "Epoch 95: MSE = 0.1078\n",
            "Epoch 96: MSE = 0.1052\n",
            "Epoch 97: MSE = 0.1026\n",
            "Epoch 98: MSE = 0.1001\n",
            "Epoch 99: MSE = 0.0976\n",
            "Epoch 100: MSE = 0.0952\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weights: [2.1249575262518463, 2.9186542641412547, -0.21332545237288644]\n",
            "Bias: 4.642198287790697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron for Linear Function with n Features"
      ],
      "metadata": {
        "id": "6pNow1I0H_ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_perceptron_linear(X, Y, lr=0.01, epochs=100):\n",
        "    n_features = len(X[0])\n",
        "    weights = [0.0] * n_features\n",
        "    bias = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = sum(X[i][j] * weights[j] for j in range(n_features)) + bias\n",
        "            error = Y[i] - y_pred\n",
        "            # Update weights and bias\n",
        "            for j in range(n_features):\n",
        "                weights[j] += lr * error * X[i][j]\n",
        "            bias += lr * error\n",
        "            total_error += error ** 2\n",
        "        mse = total_error / len(X)\n",
        "        print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    # Generate true random weights in [-1,1]\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n_features)]\n",
        "    true_bias = 5  # fixed bias\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(n_samples):\n",
        "        features = [random.uniform(0, 1) for _ in range(n_features)]\n",
        "        y = sum(features[i] * true_weights[i] for i in range(n_features)) + true_bias\n",
        "        X.append(features)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y, true_weights, true_bias\n",
        "\n",
        "\n",
        "# Test for n=4 and n=5\n",
        "for n in [4, 5]:\n",
        "    print(f\"\\nTraining for n={n} features:\")\n",
        "    X, Y, true_w, true_b = generate_dataset(n)\n",
        "    print(f\"True weights: {true_w}\")\n",
        "    print(f\"True bias: {true_b}\")\n",
        "    learned_weights, learned_bias = train_perceptron_linear(X, Y)\n",
        "    print(f\"Learned weights: {learned_weights}\")\n",
        "    print(f\"Learned bias: {learned_bias}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDxV4fB2IF5F",
        "outputId": "3366f3b7-0629-4a18-b260-b92bd6b33f79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for n=4 features:\n",
            "True weights: [-0.646429029401699, -0.4756570152240125, -0.9921181938127601, -0.011760454119466468]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 14.0113\n",
            "Epoch 2: MSE = 9.6776\n",
            "Epoch 3: MSE = 6.7555\n",
            "Epoch 4: MSE = 4.7843\n",
            "Epoch 5: MSE = 3.4537\n",
            "Epoch 6: MSE = 2.5546\n",
            "Epoch 7: MSE = 1.9464\n",
            "Epoch 8: MSE = 1.5340\n",
            "Epoch 9: MSE = 1.2536\n",
            "Epoch 10: MSE = 1.0621\n",
            "Epoch 11: MSE = 0.9306\n",
            "Epoch 12: MSE = 0.8395\n",
            "Epoch 13: MSE = 0.7757\n",
            "Epoch 14: MSE = 0.7302\n",
            "Epoch 15: MSE = 0.6973\n",
            "Epoch 16: MSE = 0.6727\n",
            "Epoch 17: MSE = 0.6538\n",
            "Epoch 18: MSE = 0.6388\n",
            "Epoch 19: MSE = 0.6264\n",
            "Epoch 20: MSE = 0.6159\n",
            "Epoch 21: MSE = 0.6066\n",
            "Epoch 22: MSE = 0.5983\n",
            "Epoch 23: MSE = 0.5905\n",
            "Epoch 24: MSE = 0.5832\n",
            "Epoch 25: MSE = 0.5763\n",
            "Epoch 26: MSE = 0.5696\n",
            "Epoch 27: MSE = 0.5631\n",
            "Epoch 28: MSE = 0.5568\n",
            "Epoch 29: MSE = 0.5507\n",
            "Epoch 30: MSE = 0.5447\n",
            "Epoch 31: MSE = 0.5388\n",
            "Epoch 32: MSE = 0.5330\n",
            "Epoch 33: MSE = 0.5273\n",
            "Epoch 34: MSE = 0.5217\n",
            "Epoch 35: MSE = 0.5162\n",
            "Epoch 36: MSE = 0.5108\n",
            "Epoch 37: MSE = 0.5055\n",
            "Epoch 38: MSE = 0.5003\n",
            "Epoch 39: MSE = 0.4951\n",
            "Epoch 40: MSE = 0.4900\n",
            "Epoch 41: MSE = 0.4851\n",
            "Epoch 42: MSE = 0.4802\n",
            "Epoch 43: MSE = 0.4753\n",
            "Epoch 44: MSE = 0.4706\n",
            "Epoch 45: MSE = 0.4659\n",
            "Epoch 46: MSE = 0.4613\n",
            "Epoch 47: MSE = 0.4567\n",
            "Epoch 48: MSE = 0.4522\n",
            "Epoch 49: MSE = 0.4478\n",
            "Epoch 50: MSE = 0.4435\n",
            "Epoch 51: MSE = 0.4392\n",
            "Epoch 52: MSE = 0.4350\n",
            "Epoch 53: MSE = 0.4308\n",
            "Epoch 54: MSE = 0.4267\n",
            "Epoch 55: MSE = 0.4227\n",
            "Epoch 56: MSE = 0.4187\n",
            "Epoch 57: MSE = 0.4147\n",
            "Epoch 58: MSE = 0.4109\n",
            "Epoch 59: MSE = 0.4070\n",
            "Epoch 60: MSE = 0.4033\n",
            "Epoch 61: MSE = 0.3996\n",
            "Epoch 62: MSE = 0.3959\n",
            "Epoch 63: MSE = 0.3923\n",
            "Epoch 64: MSE = 0.3887\n",
            "Epoch 65: MSE = 0.3852\n",
            "Epoch 66: MSE = 0.3817\n",
            "Epoch 67: MSE = 0.3783\n",
            "Epoch 68: MSE = 0.3749\n",
            "Epoch 69: MSE = 0.3716\n",
            "Epoch 70: MSE = 0.3683\n",
            "Epoch 71: MSE = 0.3650\n",
            "Epoch 72: MSE = 0.3618\n",
            "Epoch 73: MSE = 0.3587\n",
            "Epoch 74: MSE = 0.3555\n",
            "Epoch 75: MSE = 0.3525\n",
            "Epoch 76: MSE = 0.3494\n",
            "Epoch 77: MSE = 0.3464\n",
            "Epoch 78: MSE = 0.3434\n",
            "Epoch 79: MSE = 0.3405\n",
            "Epoch 80: MSE = 0.3376\n",
            "Epoch 81: MSE = 0.3347\n",
            "Epoch 82: MSE = 0.3319\n",
            "Epoch 83: MSE = 0.3291\n",
            "Epoch 84: MSE = 0.3263\n",
            "Epoch 85: MSE = 0.3236\n",
            "Epoch 86: MSE = 0.3209\n",
            "Epoch 87: MSE = 0.3182\n",
            "Epoch 88: MSE = 0.3156\n",
            "Epoch 89: MSE = 0.3130\n",
            "Epoch 90: MSE = 0.3104\n",
            "Epoch 91: MSE = 0.3079\n",
            "Epoch 92: MSE = 0.3054\n",
            "Epoch 93: MSE = 0.3029\n",
            "Epoch 94: MSE = 0.3004\n",
            "Epoch 95: MSE = 0.2980\n",
            "Epoch 96: MSE = 0.2956\n",
            "Epoch 97: MSE = 0.2932\n",
            "Epoch 98: MSE = 0.2909\n",
            "Epoch 99: MSE = 0.2885\n",
            "Epoch 100: MSE = 0.2862\n",
            "Learned weights: [0.4680442651089441, 1.0066434005868619, 0.258967966928057, 0.41429243788769704]\n",
            "Learned bias: 2.970409368493869\n",
            "\n",
            "Training for n=5 features:\n",
            "True weights: [-0.6156616357772862, -0.542097489165025, -0.9878588199681084, -0.9479577499901841, -0.8869602363007469]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 8.6535\n",
            "Epoch 2: MSE = 5.8413\n",
            "Epoch 3: MSE = 4.0078\n",
            "Epoch 4: MSE = 2.8116\n",
            "Epoch 5: MSE = 2.0306\n",
            "Epoch 6: MSE = 1.5200\n",
            "Epoch 7: MSE = 1.1855\n",
            "Epoch 8: MSE = 0.9657\n",
            "Epoch 9: MSE = 0.8206\n",
            "Epoch 10: MSE = 0.7243\n",
            "Epoch 11: MSE = 0.6596\n",
            "Epoch 12: MSE = 0.6156\n",
            "Epoch 13: MSE = 0.5852\n",
            "Epoch 14: MSE = 0.5635\n",
            "Epoch 15: MSE = 0.5476\n",
            "Epoch 16: MSE = 0.5355\n",
            "Epoch 17: MSE = 0.5258\n",
            "Epoch 18: MSE = 0.5179\n",
            "Epoch 19: MSE = 0.5110\n",
            "Epoch 20: MSE = 0.5048\n",
            "Epoch 21: MSE = 0.4992\n",
            "Epoch 22: MSE = 0.4939\n",
            "Epoch 23: MSE = 0.4889\n",
            "Epoch 24: MSE = 0.4840\n",
            "Epoch 25: MSE = 0.4793\n",
            "Epoch 26: MSE = 0.4748\n",
            "Epoch 27: MSE = 0.4703\n",
            "Epoch 28: MSE = 0.4659\n",
            "Epoch 29: MSE = 0.4617\n",
            "Epoch 30: MSE = 0.4574\n",
            "Epoch 31: MSE = 0.4533\n",
            "Epoch 32: MSE = 0.4492\n",
            "Epoch 33: MSE = 0.4452\n",
            "Epoch 34: MSE = 0.4413\n",
            "Epoch 35: MSE = 0.4374\n",
            "Epoch 36: MSE = 0.4336\n",
            "Epoch 37: MSE = 0.4298\n",
            "Epoch 38: MSE = 0.4261\n",
            "Epoch 39: MSE = 0.4224\n",
            "Epoch 40: MSE = 0.4188\n",
            "Epoch 41: MSE = 0.4153\n",
            "Epoch 42: MSE = 0.4118\n",
            "Epoch 43: MSE = 0.4084\n",
            "Epoch 44: MSE = 0.4050\n",
            "Epoch 45: MSE = 0.4016\n",
            "Epoch 46: MSE = 0.3984\n",
            "Epoch 47: MSE = 0.3951\n",
            "Epoch 48: MSE = 0.3919\n",
            "Epoch 49: MSE = 0.3888\n",
            "Epoch 50: MSE = 0.3857\n",
            "Epoch 51: MSE = 0.3826\n",
            "Epoch 52: MSE = 0.3796\n",
            "Epoch 53: MSE = 0.3767\n",
            "Epoch 54: MSE = 0.3737\n",
            "Epoch 55: MSE = 0.3709\n",
            "Epoch 56: MSE = 0.3680\n",
            "Epoch 57: MSE = 0.3652\n",
            "Epoch 58: MSE = 0.3625\n",
            "Epoch 59: MSE = 0.3598\n",
            "Epoch 60: MSE = 0.3571\n",
            "Epoch 61: MSE = 0.3544\n",
            "Epoch 62: MSE = 0.3518\n",
            "Epoch 63: MSE = 0.3493\n",
            "Epoch 64: MSE = 0.3467\n",
            "Epoch 65: MSE = 0.3442\n",
            "Epoch 66: MSE = 0.3418\n",
            "Epoch 67: MSE = 0.3393\n",
            "Epoch 68: MSE = 0.3369\n",
            "Epoch 69: MSE = 0.3346\n",
            "Epoch 70: MSE = 0.3322\n",
            "Epoch 71: MSE = 0.3299\n",
            "Epoch 72: MSE = 0.3277\n",
            "Epoch 73: MSE = 0.3254\n",
            "Epoch 74: MSE = 0.3232\n",
            "Epoch 75: MSE = 0.3210\n",
            "Epoch 76: MSE = 0.3189\n",
            "Epoch 77: MSE = 0.3168\n",
            "Epoch 78: MSE = 0.3147\n",
            "Epoch 79: MSE = 0.3126\n",
            "Epoch 80: MSE = 0.3106\n",
            "Epoch 81: MSE = 0.3086\n",
            "Epoch 82: MSE = 0.3066\n",
            "Epoch 83: MSE = 0.3046\n",
            "Epoch 84: MSE = 0.3027\n",
            "Epoch 85: MSE = 0.3008\n",
            "Epoch 86: MSE = 0.2989\n",
            "Epoch 87: MSE = 0.2970\n",
            "Epoch 88: MSE = 0.2952\n",
            "Epoch 89: MSE = 0.2933\n",
            "Epoch 90: MSE = 0.2916\n",
            "Epoch 91: MSE = 0.2898\n",
            "Epoch 92: MSE = 0.2880\n",
            "Epoch 93: MSE = 0.2863\n",
            "Epoch 94: MSE = 0.2846\n",
            "Epoch 95: MSE = 0.2829\n",
            "Epoch 96: MSE = 0.2812\n",
            "Epoch 97: MSE = 0.2796\n",
            "Epoch 98: MSE = 0.2780\n",
            "Epoch 99: MSE = 0.2764\n",
            "Epoch 100: MSE = 0.2748\n",
            "Learned weights: [0.5233403760905181, 0.6254665340508464, 0.3049222180482543, 0.7791979186778826, -0.1913770202743615]\n",
            "Learned bias: 2.17142515428677\n"
          ]
        }
      ]
    }
  ]
}